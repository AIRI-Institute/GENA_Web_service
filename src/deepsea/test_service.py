import inspect
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List

import numpy as np
import torch
from transformers import AutoConfig, AutoTokenizer

from src.gena_lm.utils import get_cls_by_name

service_folder = Path(__file__).parent.absolute()


@dataclass
class DeepSeaConf:
    max_batch_len = 8
    max_seq_len = 1000
    target_len = 200
    context_len = 400
    max_tokens = 192
    num_labels = 919
    tokenizer = service_folder.joinpath('data/tokenizers/human/BPE_32k/')
    model_cls = 'src.gena_lm.modeling_bert:BertForSequenceClassification'
    model_cfg = service_folder.joinpath('data/configs/L12-H768-A12-V32k-preln.json')
    checkpoint_path = service_folder.joinpath('data/checkpoints/model_best.pth')


class DeepSeaService:
    def __init__(self, config: dataclass):
        self.conf = config

        # define tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)

        # define model | multi-class sequence classification
        model_cfg = AutoConfig.from_pretrained(config.model_cfg)
        model_cfg.num_labels = config.num_labels
        model_cfg.problem_type = 'multi_label_classification'
        model_cls = get_cls_by_name(config.model_cls)
        self.model = model_cls(config=model_cfg)
        checkpoint = torch.load(config.checkpoint_path, map_location=torch.device('cpu'))
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        self.model_forward_args = set(inspect.getfullargspec(self.model.forward).args)

    @staticmethod
    def dna_string_padding(seq: str, length: int):
        pad_len = (length - len(seq)) // 2
        seq = ('N' * pad_len) + seq + ('N' * pad_len)
        if len(seq) < length:
            seq += 'N' * (length - len(seq))

        return seq

    def preprocess(self, dna_seq: str):
        batch = []
        if len(dna_seq) <= self.conf.target_len:
            batch.append(dna_seq)

        elif (len(dna_seq) > self.conf.target_len) and (len(dna_seq) <= self.conf.max_seq_len):
            batch.append(dna_seq)

        elif len(dna_seq) > self.conf.max_seq_len:
            offset = len(dna_seq) // self.conf.target_len
            if offset == 5:
                batch.append(dna_seq[:self.conf.max_seq_len])
            elif offset > 5:
                for i in range(0, offset, 1):  # todo: контекст не учитывается !
                    batch.append(dna_seq[(i * self.conf.target_len):((i + 1) * self.conf.target_len)])

        if len(batch) > self.conf.max_batch_len:
            print(f"WARNING: max_batch_len = {self.conf.max_batch_len}, but {len(batch)} was found. "
                  f"Service will annotate only first {self.conf.max_batch_len} elements of batch;")
            batch = batch[:self.conf.max_batch_len]

        return batch

    def create_batch(self, examples: List[str]):
        batch = {'input_ids': [],
                 'token_type_ids': [],
                 'attention_mask': [],
                 'labels': None,
                 "labels_ohe": None,
                 'labels_mask': None}

        for dna_seq in examples:
            features = self.tokenizer(dna_seq,
                                      add_special_tokens=True,
                                      padding='max_length',
                                      truncation='longest_first',
                                      max_length=self.conf.max_tokens,
                                      return_tensors='np')
            # return features generated by tokenizer and labels
            for fn in features:
                features[fn] = features[fn][0]

            batch['input_ids'].append(features['input_ids'])
            batch['token_type_ids'].append(features['token_type_ids'])
            batch['attention_mask'].append(features['attention_mask'])

        batch['input_ids'] = torch.from_numpy(np.vstack(batch['input_ids'])).int()
        batch['token_type_ids'] = torch.from_numpy(np.vstack(batch['token_type_ids'])).int()
        batch['attention_mask'] = torch.from_numpy(np.vstack(batch['attention_mask'])).float()

        return batch

    def __call__(self, dna_examples) -> Dict:
        batch = self.preprocess(dna_examples)
        batch = self.create_batch(batch)
        model_out = self.model(**{k: batch[k] for k in batch if k in self.model_forward_args})

        predictions = torch.sigmoid(model_out['logits']).detach().numpy()
        labels = np.where(predictions < 0.5, predictions, 1)
        labels = np.where(predictions > 0.5, labels, 0)
        input_ids = batch['input_ids'].detach().numpy().flatten()

        service_response = dict()
        service_response['prediction'] = labels
        service_response['seq'] = self.tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)

        return service_response


if __name__ == "__main__":
    conf = DeepSeaConf()
    instance_class = DeepSeaService(conf)

    example = 'TTTACTTTTAACATTTTGAAATATAAGACACCTAGAAAAAAGTTCACAGAAGGTAAATGTACACTTAAACAAATAAAGTGAGCACCCAAGTAGACACCACTTGAGCCAAGAACTGGAACATTACCAGCACCCCAGAAGCCCGATGGTATTCTTTCCCTATGGCAGCCCGTCTCAGAAAACAACCTTCCTCTCCCAGAAGATTTACTTTTAACATTTTGAAATATAAGACACCTAGAAAAAAGTTCACAGAAGGTAAATGTACACTTAAACAAATAAAGTGAGCACCCAAGTAGACACCACTTGAGCCAAGAACTGGAACATTACCAGCACCCCAGAAGCCCGATGGTATTCTTTCCCTATGGCAGCCCGTCTCAGAAAACAACCTTCCTCTCCCAGAAGATTTACTTTTAACATTTTGAAATATAAGACACCTAGAAAAAAGTTCACAGAAGGTAAATGTACACTTAAACAAATAAAGTGAGCACCCAAGTAGACACCACTTGAGCCAAGAACTGGAACATTACCAGCACCCCAGAAGCCCGATGGTATTCTTTCCCTATGGCAGCCCGTCTCAGAAAACAACCTTCCTCTCCCAGAAGATTTACTTTTAACATTTTGAAATATAAGACACCTAGAAAAAAGTTCACAGAAGGTAAATGTACACTTAAACAAATAAAGTGAGCACCCAAGTAGACACCACTTGAGCCAAGAACTGGAACATTACCAGCACCCCAGAAGCCCGATGGTATTCTTTCCCTATGGCAGCCCGTCTCAGAAAACAACCTTCCTCTCCCAGAAGATTTACTTTTAACATTTTGAAATATAAGACACCTAGAAAAAAGTTCACAGAAGGTAAATGTACACTTAAACAAATAAAGTGAGCACCCAAGTAGACACCACTTGAGCCAAGAACTGGAACATTACCAGCACCCCAGAAGCCCGATGGTATTCTTTCCCTATGGCAGCCCGTCTCAGAAAACAACCTTCCTCTCCCAGAAGATTTACTTTTAACATTTTGAAATATAAGACACCTAGAAAAAAGTTCACAGAAGGTAAATGTACACTTAAACAAATAAAGTGAGCACCCAAGTAGACACCACTTGAGCCAAGAACTGGAACATTACCAGCACCCCAGAAGCCCGATGGTATTCTTTCCCTATGGCAGCCCGTCTCAGAAAACAACCTTCCTCTCCCAGAAGA'
    y = instance_class(example)

    print(y['prediction'].shape)
    print(y['prediction'])
    print(len(y['seq']), y['seq'])
