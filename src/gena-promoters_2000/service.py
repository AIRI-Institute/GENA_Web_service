import inspect
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List

import numpy as np
import torch
from transformers import AutoConfig, AutoTokenizer
from captum.attr import LayerIntegratedGradients # type: ignore
from gena_lm.utils import get_cls_by_name
import logging
logging.basicConfig(format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)
service_folder = Path(__file__).parent.absolute()
import pandas as pd 

def summarize_attributions(attributions):
        attributions = attributions.sum(dim=-1).squeeze(0)
        attributions = attributions / torch.norm(attributions)
        return attributions

def token_positions(seq_tok):
    if len(seq_tok._encodings) != 1:
        raise Exception("Unexpected number of encodings")
    
    prev_e = 0
    ind2pos = []
    for ind, (_, e) in enumerate(seq_tok._encodings[0].offsets):
        ind2pos.append( (prev_e, e))
        prev_e = e
    return ind2pos
            

@dataclass
class PromotersConf:
    bpe_dropout = 0.0
    working_segment = 2000
    segment_step = 1000
    batch_size = 1
    max_tokens = 512
    tokenizer = service_folder.joinpath('data/tokenizers/t2t_1000h_multi_32k/')
    model_cls = 'gena_lm.modeling_bert:BertForSequenceClassification'
    model_cfg = service_folder.joinpath('data/configs/L24-H1024-A16-V32k-preln-lastln.json')
    checkpoint_path = service_folder.joinpath('data/checkpoints/model_best.pth')
    base_model = "bert_large_512_lastln_t2t_1000G_bs256_lr_1e-04_fp16-1750k_iters"
    attr_steps: int = 10



class PromotersService:
    def __init__(self, config: dataclass):
        self.conf = config
        # define tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer)
        if (config.bpe_dropout is not None) and (config.bpe_dropout > 0.0):
            if hasattr(self.tokenizer._tokenizer.model, 'dropout'):
                self.tokenizer._tokenizer.model.dropout = config.bpe_dropout
            else:
                print('BPE dropout is not set as tokenizer does not support it.')

        # define model | sequence binary classification
        model_cfg = AutoConfig.from_pretrained(config.model_cfg)
        model_cfg.num_labels = 2
        model_cls = get_cls_by_name(config.model_cls)
        print(f'[ Using model class: {model_cls} ]')
        self.model = model_cls(config=model_cfg)

        # load model checkpoint
        checkpoint = torch.load(config.checkpoint_path, map_location=torch.device('cpu'))
        self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        self.model.eval()
        self.model_forward_args = set(inspect.getfullargspec(self.model.forward).args)

    def preprocess(self, dna_seq: str):
        features = self.tokenizer(dna_seq,
                                  add_special_tokens=True,
                                  padding='max_length',
                                  truncation='longest_first',
                                  max_length=self.conf.max_tokens,
                                  return_tensors='np')
        # return features generated by tokenizer and labels
        for fn in features:
            features[fn] = features[fn][0]

        return features

    @staticmethod
    def create_batch(seq_list: List[Dict]) -> Dict:
        batch = {'input_ids': [],
                 'token_type_ids': [],
                 'attention_mask': [],
                 'labels': None,
                 "labels_ohe": None,
                 'labels_mask': None}

        for features in seq_list:
            batch['input_ids'].append(features['input_ids'])
            batch['token_type_ids'].append(features['token_type_ids'])
            batch['attention_mask'].append(features['attention_mask'])

        batch['input_ids'] = torch.from_numpy(np.vstack(batch['input_ids'])).int()
        batch['token_type_ids'] = torch.from_numpy(np.vstack(batch['token_type_ids'])).int()
        batch['attention_mask'] = torch.from_numpy(np.vstack(batch['attention_mask'])).float()

        return batch

    def __call__(self, queries, temp_storage: Path, calc_importance: bool) -> Dict:
        # preprocessing
        samples = []
        for query in queries:
            samples.append(self.preprocess(query['seq']))

        # model inference
        batch = self.create_batch(samples)
        with torch.inference_mode():
            model_out = self.model(**{k: batch[k] for k in batch if k in self.model_forward_args})
        # postprocessing
        service_response = dict()
        # write predictions
        predictions = torch.argmax(model_out['logits'].detach(), dim=-1).numpy()
        service_response['prediction'] = predictions  # [bs,]
        # write tokens
        service_response['seq'] = []
        input_ids = batch['input_ids'].detach().numpy()
        for batch_element in input_ids:
            service_response['seq'].append(self.tokenizer.convert_ids_to_tokens(batch_element,
                                                                                skip_special_tokens=True))
        
        if calc_importance:
            attributions = self.annotate_predictions(samples, predictions, queries, temp_storage)
            service_response['attr'] = attributions

        service_response['queries'] = queries

        return service_response


    def create_attr_object(self):
        def predict_classifier(inputs, 
                               token_type_ids=None, 
                               attention_mask=None):
            assert token_type_ids is not None
            assert attention_mask is not None
            output = self.model(inputs, 
                   token_type_ids=token_type_ids, 
                   attention_mask=attention_mask)
            return output.logits
        lig_object = LayerIntegratedGradients(predict_classifier, self.model.bert.embeddings)
        return lig_object

    def annotate_predictions(self, samples, labels, dna_queries, temp_storage):
        lig_object = self.create_attr_object()
        attributions = []
        for si, smpl in enumerate(samples):
            smpl_attrs = {}
            lab = labels[si]
            if lab == 1: # predicted to be promoter
                logger.info(f"Creating attributions for sample {si}")
                attr = self.annotate_sample(lig_object=lig_object, sample=smpl, target=lab, query=dna_queries[si])
                temp_path = temp_storage / f"attr_sample{si}_target{lab}"
                attr.to_csv(temp_path, sep="\t", index=False)
                smpl_attrs[lab] = temp_path

            attributions.append(smpl_attrs)
        return attributions

    def annotate_sample(self, lig_object, sample, target, query):
        presample = sample
        sample = {
           "input_ids": torch.LongTensor(sample['input_ids']).unsqueeze(0),
           "token_type_ids": torch.LongTensor(sample['token_type_ids']).unsqueeze(0),
           "attention_mask": torch.LongTensor(sample['attention_mask']).unsqueeze(0),
        }

        attributions = lig_object.attribute(inputs=(sample['input_ids'],
                                            sample['token_type_ids'],
                                            sample['attention_mask']),
                                    target=int(target),
                                    n_steps=self.conf.attr_steps,
                                    return_convergence_delta=False)
        

        attributions = attributions[:, 1:-1, :] # remove CLS and SEP
        attributions = summarize_attributions(attributions).cpu()

        bed_like_table = {'tok_pos': [], 'token': [], 'attr': [], 'start': [], 'end': []}
        
        pretokens = self.tokenizer.convert_ids_to_tokens(presample['input_ids'], skip_special_tokens=False)
        tokens = pretokens

        startends = token_positions(presample)
        
        for i, tok in enumerate(tokens):
            start, end = startends[i]
            if start >= end:
                continue # special token 
            attr = attributions[i].item()
            bed_like_table['tok_pos'].append(i)
            bed_like_table['token'].append(tok)
            bed_like_table['attr'].append(attr)
           
            bed_like_table['start'].append(start)
            bed_like_table['end'].append(end)

        seq_len = len(query['seq'])
        df = pd.DataFrame(bed_like_table)
        
        df = df[np.logical_and(df['start'] >= query['lpad'],
                               df['end'] <= (seq_len - query['rpad'])
                              )]
        df['start'] = df['start'] + query['context_start']
        df['end'] = df['end'] + query['context_start']
        
        return df 

